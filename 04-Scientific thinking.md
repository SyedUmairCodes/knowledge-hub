Types of Data Analysis: The document describes six categories of data analysis, ordered by complexity1:

○

Descriptive Analysis: This type of analysis aims to summarize data using measures like mean, median, mode, range, and standard deviation. It focuses on describing the sample without generalizing to a larger population1.

○

Exploratory Analysis: This involves examining data to find relationships between variables. It can reveal correlations but doesn't establish causation. It helps formulate hypotheses for further study12.

○

Inferential Analysis: This uses a sample of data to make inferences about a larger population. It involves statistical modeling and is dependent on a representative sample23.

○

Predictive Analysis: This uses current and historical data to predict future outcomes. While it can identify correlations, it doesn't imply causation. The accuracy depends on the variables measured and the model used3.

○

Causal Analysis: This aims to identify cause-and-effect relationships by manipulating variables. It is often done through randomized studies and is considered the gold standard in data analysis, though getting appropriate data is challenging4.

○

Mechanistic Analysis: This seeks to understand the exact changes in variables that lead to specific outcomes. It is most often applied in physical or engineering sciences, or biological sciences when the datasets are well-modeled by deterministic equations5.

●

Experimental Design: The source emphasizes the importance of good experimental design for answering data science questions effectively6. Key concepts in experimental design include78:

○

Independent and Dependent Variables: The independent variable is manipulated by the experimenter, while the dependent variable is expected to change as a result7.

○

Hypothesis: An educated guess about the relationship between variables7.

○

Sample Size: The number of experimental subjects included in the experiment. The sample size should be large enough to accurately represent the population of interest7.

○

Confounders: Extraneous variables that may affect the relationship between dependent and independent variables7.

○

Control Groups: A group of experimental subjects that are not manipulated, used for comparison8.

○

Blinding: A method to control for the placebo effect by keeping subjects unaware of their assigned treatment group8.

○

Randomization: Randomly assigning individuals to groups to balance confounders and reduce systematic errors8.

○

Replication: Repeating an experiment with different subjects to verify results and measure data variability89.

●

Sharing Data and Code: The document advocates for sharing data and code, often on platforms like GitHub, to promote transparency and reproducibility9.

●

P-value and P-hacking: The document discusses p-values, which indicate the probability that results occurred by chance, and warns against p-hacking (manipulating data to achieve statistical significance)910.

●

Big Data: The source also addresses the concept of Big Data, characterized by volume, velocity, and variety11.

○

Structured vs. Unstructured Data: Big Data often involves unstructured data from sources like emails, social media, and GPS tracking. Analyzing unstructured data poses challenges, but it can offer valuable insights11.

○

Challenges of Big Data: These include storing and analyzing large volumes of data, dealing with constantly updating information, managing the variety of data sources, and working with messy, unstructured data11.

○

Benefits of Big Data: These include identifying hidden correlations, providing real-time information, and answering questions that were previously inaccessible11.

○

The Importance of Relevant Data: The document emphasizes that even with Big Data, the data must be relevant to the question being asked11.